<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Introduction to Reinforcement Learning – Autonomous and Adaptive Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./sec3.html" rel="next">
<link href="./sec1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a34d670291f06f286357e447776a572a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./sec2.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Reinforcement Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Autonomous and Adaptive Systems</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Intelligent Agents and Machines</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Reinforcement Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Temporal Difference Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Deep Learning and Neural Architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Value Approximation Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Monte Carlo Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generative Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sec10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Multi-agent Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#examples-of-problems" id="toc-examples-of-problems" class="nav-link active" data-scroll-target="#examples-of-problems"><span class="header-section-number">2.1</span> Examples of Problems</a></li>
  <li><a href="#finite-markov-decision-processes" id="toc-finite-markov-decision-processes" class="nav-link" data-scroll-target="#finite-markov-decision-processes"><span class="header-section-number">2.2</span> Finite Markov Decision Processes</a></li>
  <li><a href="#goals-and-rewards" id="toc-goals-and-rewards" class="nav-link" data-scroll-target="#goals-and-rewards"><span class="header-section-number">2.3</span> Goals and Rewards</a></li>
  <li><a href="#policies-and-value-functions" id="toc-policies-and-value-functions" class="nav-link" data-scroll-target="#policies-and-value-functions"><span class="header-section-number">2.4</span> Policies and Value Functions</a></li>
  <li><a href="#choosing-the-rewards" id="toc-choosing-the-rewards" class="nav-link" data-scroll-target="#choosing-the-rewards"><span class="header-section-number">2.5</span> Choosing the Rewards</a></li>
  <li><a href="#estimating-value-functions" id="toc-estimating-value-functions" class="nav-link" data-scroll-target="#estimating-value-functions"><span class="header-section-number">2.6</span> Estimating Value Functions</a></li>
  <li><a href="#optimal-policies-and-optimal-value-functions" id="toc-optimal-policies-and-optimal-value-functions" class="nav-link" data-scroll-target="#optimal-policies-and-optimal-value-functions"><span class="header-section-number">2.7</span> Optimal Policies and Optimal Value Functions</a></li>
  <li><a href="#difference-between-rl-and-other-ml-sub-domains" id="toc-difference-between-rl-and-other-ml-sub-domains" class="nav-link" data-scroll-target="#difference-between-rl-and-other-ml-sub-domains"><span class="header-section-number">2.8</span> Difference between RL and other ML sub-domains</a></li>
  <li><a href="#when-to-use-rl" id="toc-when-to-use-rl" class="nav-link" data-scroll-target="#when-to-use-rl"><span class="header-section-number">2.9</span> When to use RL</a></li>
  <li><a href="#rl-applications" id="toc-rl-applications" class="nav-link" data-scroll-target="#rl-applications"><span class="header-section-number">2.10</span> RL Applications</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">2.11</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Reinforcement Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<ul>
<li><p><strong>Key idea:</strong> a natural way of thinking about learning is learning through interaction with the external world.</p></li>
<li><p>Learning from interation is a foundational idea underlying nearly all theories of learning and intelligence.</p></li>
<li><p>Reinforcement learning is learning what to do - how to map situations to actions - so as to maximise a numerical reward.</p>
<ul>
<li><strong><em>Goal-directed</em></strong> learning from interaction.</li>
</ul></li>
<li><p>The learner is not told which actions to take, but instead it must discover which actions yield the most reward by trying them.</p></li>
</ul>
<section id="examples-of-problems" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="examples-of-problems"><span class="header-section-number">2.1</span> Examples of Problems</h2>
<p align="center">
<img src="images/go.png" width="40%"> <img src="images/robot.png" width="52%">
</p>
</section>
<section id="finite-markov-decision-processes" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="finite-markov-decision-processes"><span class="header-section-number">2.2</span> Finite Markov Decision Processes</h2>
<p><strong>Markov Decision Processes</strong> (<strong>MDPs</strong>) are a mathematically idealised formulation of Reinforcement Learning for which precise theoretical statements can be made.</p>
<ul>
<li>Tension between breadth of applicability and mathematical tractability.</li>
<li>MDPs provide a way for framing the problem of learning from experience, and, more specifically, from interacting with an environment.</li>
</ul>
<h3 class="anchored">
Definitions
</h3>
<p>Reinforcement Learning, or RL for short, is a unique facet of machine learning where <u>an agent learns to make decisions through trial and error</u>.</p>
<p>Two main entities:</p>
<p align="center">
<img src="images/agent_environment_1.png" width="55%">
</p>
<ol type="1">
<li><p><strong>Agent</strong> = learner and decision-maker.</p>
<ul>
<li>Interacts with the environment selecting <strong><em>actions</em></strong>.</li>
<li><strong><em>Observes</em></strong> and acts within the environment.</li>
<li>Receives:
<ul>
<li><strong><em>rewards</em></strong> for good decisions.</li>
<li><strong><em>penalties</em></strong> for bad decisions.</li>
</ul></li>
</ul></li>
<li><p><strong>Environment</strong> = everything else outside the agent.</p>
<ul>
<li>Changes following actions of the agent.</li>
</ul></li>
</ol>
<ul>
<li><strong>Goal</strong>: devise a strategy that <u>maximises the total reward over time</u> .</li>
</ul>
<h3 class="anchored">
RL framework
</h3>
<ul>
<li><strong>Agent</strong>: learner, decision-maker.</li>
<li><strong>Environment</strong>: challenges to be solved.</li>
<li><strong>State</strong>: environment snapshot at given time.</li>
<li><strong>Action</strong>: agent’s choice in response to state.</li>
<li><strong>Reward</strong>: feedback for agent action (positive or negative).</li>
</ul>
<p>
<img src="images/rl_framework.png" width="45%">
</p>
<h3 class="anchored">
RL interaction loop
</h3>
<ul>
<li><p>The agent and the environment interact at each discrete step of a sequence <span class="math inline">\(t=0,1,2,3,\dots\)</span></p></li>
<li><p>At each time step <span class="math inline">\(t\)</span>, the agent receives some representation of the environment <strong>state</strong> <span class="math inline">\(S_t \in \mathcal{S}\)</span> where <span class="math inline">\(\mathcal{S}\)</span> is the set of the states.</p></li>
<li><p>On that basis, an agent selects an <strong>action</strong> <span class="math inline">\(A_t \in \mathcal{A}(S_t)\)</span> where <span class="math inline">\(\mathcal{A}(S_t)\)</span> is the set of the actions that can be taken in state <span class="math inline">\(S_t\)</span>.</p></li>
<li><p>At time <span class="math inline">\(t+1\)</span>, as a consequence of its action, the agent receives a <strong>reward</strong> <span class="math inline">\(R_{t+1} \in \mathcal{R}\)</span>, where <span class="math inline">\(\mathcal{R}\)</span> is the set of rewards (expressed as real numbers).</p></li>
</ul>
<p align="center">
<img src="images/agent_environment_2.png" width="55%">
</p>
<p>Let’s demonstrate the agent-environment interaction using a generic code example. The process starts by creating an environment and retrieving the initial state. The agent then enters a loop where it selects an action based on the current state in each iteration. After executing the action, the environment provides feedback in the form of a new state and a reward. Finally, the agent updates its knowledge based on the state, action, and reward it received.</p>
<div id="39aae17b" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> create_environment()</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> env_get_initial_state()</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> choose_action(state)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    state, reward <span class="op">=</span> env_execute(action)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    update_knowledge(state, action, reward)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="goals-and-rewards" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="goals-and-rewards"><span class="header-section-number">2.3</span> Goals and Rewards</h2>
<ul>
<li><p>The goal of the agent is formalised in terms of the reward it receives.</p></li>
<li><p>At each time step, the reward is a simple number <span class="math inline">\(R_t \in \mathbb{R}\)</span>.</p></li>
<li><p>Informally, the agent’s goal is to maximise the total amount it receives.</p></li>
<li><p>The agent should not maximise the immediate reward, but the <strong>cumulative reward</strong>.</p></li>
</ul>
<h3 class="anchored">
The Reward Hypothesis
</h3>
<p>We can formalise the goal of an agent by stating the “reward hypothesis”:</p>
<blockquote class="blockquote">
<p>All of what we mean by goals and purposes can be well thought of as the maximisation of the exprected value of the cumulative sum of a received scalar signal (reward).</p>
</blockquote>
<h3 class="anchored">
Expected Returns
</h3>
<p>In RL, actions carry long-term consequences, impacting both immediate and future rewards. The agent’s goal goes beyond maximizing immediate gains; it receives a sequence of rewards and it strives to accumulate the highest <em>total reward over time</em>. This leads us to a key concept in RL: the <strong>expected return</strong>.</p>
<blockquote class="blockquote">
<p>The <em>expected return <span class="math inline">\(G_t\)</span></em> is a function of the reward sequence <span class="math inline">\(R_{t+1}, R_{t+2}, R_{t+3}, \dots\)</span></p>
<blockquote class="blockquote">
<p><span class="math inline">\(G_t\)</span> is the sum of all rewards the agent expects to accumulate throughout its journey.</p>
</blockquote>
</blockquote>
<p align="center">
<img src="images/return.png" width="42%">
</p>
<p>Accordingly, the agent learns to anticipate the sequence of actions that will yield the highest possible return.</p>
<h3 class="anchored">
Episodic Tasks and Continuing Tasks
</h3>
<p>In RL, we encounter two types of tasks: <em>episodic</em> and <em>continuous</em>.</p>
<p><strong>Episodic tasks</strong> are those in which we can identify a final step of the sequence of rewards, i.e.&nbsp;in which the interaction between the agent and the environment can be broken into sub-sequences that we call <strong><em>episodes</em></strong> (such as play of a game, rpeeated tasks, etc.). For example, in a chess game played by an agent, each game constitutes an episode; once a game concludes, the environment resets for the next one.</p>
<blockquote class="blockquote">
<p>An <em>episodic task</em> is divided into distinct episodes, each with a defined beginning and end.</p>
<blockquote class="blockquote">
<p>Each <em>episode</em> ends in terminal state after <span class="math inline">\(T\)</span> steps, followed by a reset to a standard starting state or to a sample of a distribution of starting states.</p>
</blockquote>
<blockquote class="blockquote">
<p>The <em>next episode</em> is completely independent from the previous one.</p>
</blockquote>
</blockquote>
<p>On the other hand, <strong>continuing tasks</strong> involve <strong><em>ongoing interaction</em></strong> without distinct episodes (e.g.&nbsp;ongoing process control or robots with a long-lifespan). A typical example is an agent continuously adjusting traffic lights in a city to optimize flow.</p>
<blockquote class="blockquote">
<p>A <em>continuing task</em> is one in which it is not possible to identify a final state.</p>
</blockquote>
<h3 class="anchored">
Expected Return for Episodic Tasks and Continuing Tasks
</h3>
<blockquote class="blockquote">
<p>In the case of <strong><em>episodic tasks</em></strong> the expected return associated to the selection of an action <span class="math inline">\(A_t\)</span> is the sum of rewards defined as follows:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T\)</span></p>
</blockquote>
</blockquote>
<blockquote class="blockquote">
<p>In the case of <strong><em>continuing tasks</em></strong> the expected return associated to the selection of an action <span class="math inline">\(A_t\)</span> is defined as follows:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots  = \sum_{k=0}^\infty \gamma^k R_{t+k+1}\)</span></p>
</blockquote>
<p>where <span class="math inline">\(\gamma\)</span> is the discount rate, with <span class="math inline">\(0 \leq \gamma \leq 1\)</span>.</p>
</blockquote>
<h3 class="anchored">
Discounting Rewards
</h3>
<p>Immediate rewards are typically valued more than future ones, leading to the concept of <strong>discounted return</strong>. This concept prioritizes more recent rewards by multiplying each reward by a discount factor, gamma, raised to the power of its respective time step. For example, for expected rewards r1 through rn, the discounted return would be calculated as r1 + gamma * r2 + gamma^2 * r3, and so on.</p>
<p align="center">
<img src="images/discounted_return.png" width="65%">
</p>
<p>The <strong>discount factor</strong> <span class="math inline">\(\gamma\)</span>, ranging between 0 and 1, is crucial for balancing immediate and long-term rewards. A lower gamma value leads the agent to prioritize immediate gains, while a higher value emphasizes long-term benefits. At the extremes, a gamma of zero means the agent focuses solely on immediate rewards, while a gamma of one considers future rewards as equally important, applying no discount.</p>
<p align="center">
<img src="images/discount_factor.png" width="60%">
</p>
<h4 class="anchored">
Numerical example:
</h4>
<p>In this example, we’ll demonstrate how to calculate the discounted_return from an array of <code>expected_rewards</code>. We define a <code>discount_factor</code> of 0.9, then create an array of discounts, where each element corresponds to the discount factor raised to the power of the reward’s position in the sequence.</p>
<div id="0acbd54b" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>expected_rewards <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">3</span>])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>discount_factor <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>discounts <span class="op">=</span> np.array([discount_factor <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(expected_rewards))])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Discounts: </span><span class="sc">{</span>discounts<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Discounts: [1.   0.9  0.81]</code></pre>
</div>
</div>
<p>As we can see, discounts decrease over time, giving less importance to future rewards. Next, we multiply each reward by its corresponding discount and sum the results to compute the <code>discounted_return</code>, which is 8.83 in this example.</p>
<div id="3a127bf6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>discounted_return <span class="op">=</span> np.<span class="bu">sum</span>(expected_rewards <span class="op">*</span> discounts)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The discounted return is </span><span class="sc">{</span>discounted_return<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The discounted return is 8.83</code></pre>
</div>
</div>
<h3 class="anchored">
Relation between Returns at Successive Time Steps
</h3>
<blockquote class="blockquote">
<p>Returns at successive time steps are related to each others as follows:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots\)</span><br> <span class="math inline">\(\quad\;\; = R_{t+1} + \gamma (R_{t+1} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots )\)</span><br> <span class="math inline">\(\quad\;\; = R_{t+1} + \gamma G_{t+1}\)</span></p>
</blockquote>
</blockquote>
</section>
<section id="policies-and-value-functions" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="policies-and-value-functions"><span class="header-section-number">2.4</span> Policies and Value Functions</h2>
<p>Almost all reinforcement learning algorithms involve estimating value functions, i.e., functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).</p>
<p>A policy is used to model the behaviour of the agent based on the previous experience and the rewards (and consequently the expected returns) an agent received in the past.</p>
<h3 class="anchored">
Policy
</h3>
<p>Formally, a <em>policy</em> is a mapping from states to probabilities of each possible action, i.e.&nbsp;policy <span class="math inline">\(\pi\)</span> is a <em>probability distribution</em>.</p>
<blockquote class="blockquote">
<p>If the agent is following policy <span class="math inline">\(\pi\)</span> at time <span class="math inline">\(t\)</span>, then <span class="math inline">\(\pi(a \vert s)\)</span> is the probability that <span class="math inline">\(A_t = a\)</span> if <span class="math inline">\(S_t = s\)</span>.</p>
</blockquote>
<h3 class="anchored">
State-Value Function
</h3>
<p>The value function of a state <span class="math inline">\(s\)</span> under a policy <span class="math inline">\(\pi\)</span>, denoted <span class="math inline">\(v_\pi(s)\)</span>, is the expected return when starting in <span class="math inline">\(s\)</span> and following <span class="math inline">\(\pi\)</span> thereafter.</p>
<blockquote class="blockquote">
<p>For MDPs, we can define the <em>state-value function</em> <span class="math inline">\(v_\pi\)</span> for policy <span class="math inline">\(\pi\)</span> formally as:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(v_s \doteq E_\pi[G_t \vert S_t = s] = E_\pi [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \vert S_t = s]\)</span></p>
</blockquote>
<p>for all <span class="math inline">\(s \in \mathcal{S}\)</span>.</p>
<p>where <span class="math inline">\(E_\pi[.]\)</span> denotes the expected value of a random variable, given that the agent follows <span class="math inline">\(\pi\)</span> and <span class="math inline">\(t\)</span> is any time step. The value of the terminal state is 0.</p>
</blockquote>
<div id="93ff93a1" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>choose <span class="op">=</span> rand(<span class="fl">0.1</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> choose <span class="op">&gt;=</span> <span class="fl">0.7</span>:</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    move left</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    move right</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<h3 class="anchored">
Action-Value Function
</h3>
</section>
<section id="choosing-the-rewards" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="choosing-the-rewards"><span class="header-section-number">2.5</span> Choosing the Rewards</h2>
<iframe src="papers/2__1__Steps_Toward_Artificial_Intelligence.pdf" width="100%" height="400px">
</iframe>
<p align="center">
<em>Marvin Minsky. Steps Toward Artificial Intelligence. Proceedings of the IRE. Volume 49. Issue 1. January 1961.</em>
</p>
<h3 class="anchored">
Examples of Rewards
</h3>
</section>
<section id="estimating-value-functions" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="estimating-value-functions"><span class="header-section-number">2.6</span> Estimating Value Functions</h2>
<h3 class="anchored">
Monte-Carlo Methods
</h3>
<h3 class="anchored">
Using function approximators (neural networks and deep RL)
</h3>
</section>
<section id="optimal-policies-and-optimal-value-functions" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="optimal-policies-and-optimal-value-functions"><span class="header-section-number">2.7</span> Optimal Policies and Optimal Value Functions</h2>
<h3 class="anchored">
Definition of Optimal Policy
</h3>
<h3 class="anchored">
Optimal Value Functions
</h3>
</section>
<section id="difference-between-rl-and-other-ml-sub-domains" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="difference-between-rl-and-other-ml-sub-domains"><span class="header-section-number">2.8</span> Difference between RL and other ML sub-domains</h2>
<p>RL differs significantly from other types of machine learning, such as supervised and unsupervised learning.</p>
<p>In <strong><em>supervised learning</em></strong>, models are trained using labeled data, learning to predict outcomes based on examples. It is suitable for solving problems like classification and regression.</p>
<p><strong><em>Unsupervised learning</em></strong>, on the other hand, involves learning to identify patterns or structures from unlabeled data. It is suitable for solving problems like clustering or association analysis.</p>
<p><strong><em>Reinforcement Learning</em></strong>, distinct from both, does not use any training data, and learns through trial and error to perform actions that maximize the reward, making it ideal for decision-making tasks.</p>
<p align="center">
<img src="images/rl_vs_ml_types.png" width="100%">
</p>
<h3 class="anchored">
RL vs.&nbsp;Supervised Learning
</h3>
<p><em>Supervised learning</em> is learning from a set of labeled examples and, in interactive problems, it is hard to obtain labels in the first place. Therefore, in “unknown” situations, agents have to learn from their experience. In these situations <em>Reinforcement learning</em> is most beneficial.</p>
<h3 class="anchored">
RL vs.&nbsp;Unsupervised Learning
</h3>
<p><em>Unsupervised learning</em> is learning from datasets containing unlabelled data. Since RL does not rely on examples (labels) of correct behaviour and instead explored and learns it, we may think that RL is a type of unsupervised learning. However, this is not the case because in <em>Reinforcement Learning</em> the goal is to maximise a reward signal instead of trying to find a hidden structure.</p>
<p>For this reason, <em>Reinforcement Learning</em> is usually considred a third paradigm in addition to supervised and unsupervised learning.</p>
<p align="center">
<img src="images/supervised_unsupervised_reinforcement_learning.png" width="100%">
</p>
</section>
<section id="when-to-use-rl" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="when-to-use-rl"><span class="header-section-number">2.9</span> When to use RL</h2>
<p>In particular, RL is well-suited for scenarios that require training a model to make sequential decisions where each decision influences future observations. In this setting, the agent learns through rewards and penalties. These guide it towards developing more effective strategies without any kind of direct supervision.</p>
<ul>
<li><p>Sequential decision-making</p>
<ul>
<li>Decisions influence future observations</li>
</ul></li>
<li><p>Learning through rewards and penalties</p>
<ul>
<li>No direct supervision</li>
</ul></li>
</ul>
<h3 class="anchored">
Appropriate vs.&nbsp;Inappropriate for RL
</h3>
<p>An appropriate example for RL is <strong>playing video games</strong>, where the player needs to make sequential decisions such as jumping over obstacles or avoiding enemies. The player learns and improves by trial and error, receiving points for successful actions and losing lives for mistakes. The goal is to maximize the score by learning the best strategies to overcome the game’s challenges.</p>
<p><img src="images/playing_video_games.png" width="60%"></p>
<ul>
<li>Player makes sequential decisions.</li>
<li>Receives points and loses lives depending on actions.</li>
</ul>
<p>Conversely, RL is unsuitable for tasks such as <strong>in-game object recognition</strong>, where the objective is to identify and classify elements like characters or items in a video frame. This task does not involve sequential decision-making or interaction with an environment. Instead, supervised learning, which employs labeled data to train models in recognizing and categorizing objects, proves more effective for this purpose.</p>
<p><img src="images/in-game_object_recognition.png" width="60%"></p>
<ul>
<li>No sequential decision-making.</li>
<li>No interaction with an environment.</li>
</ul>
<p align="center">
<img src="images/appropriate_vs_inappropriate_for_rl.png" width="100%">
</p>
</section>
<section id="rl-applications" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="rl-applications"><span class="header-section-number">2.10</span> RL Applications</h2>
<p>Beyond its well-known use in gaming, RL has a myriad of applications across various sectors.</p>
<p style="display: flex; justify-content: space-around; align-items: center;">
<img src="images/robotics.png" width="20%"> <img src="images/finance.png" width="20%"> <img src="images/autonomous_vehicles.png" width="20%"> <img src="images/chatbot_development.png" width="20%">
</p>
<ol type="1">
<li><p><strong>Robotics.</strong> In robotics, RL is pivotal for teaching robots tasks through trial and error, like walking or object manipulation.</p>
<ul>
<li>Robot walking</li>
<li>Object manipulation</li>
</ul></li>
<li><p><strong>Finance.</strong> The finance industry leverages RL for optimizing trading and investment strategies to maximize profit.</p>
<ul>
<li>Optimizing trading and investment</li>
<li>Maximise profit</li>
</ul></li>
<li><p><strong>Autonomous Vehicles.</strong> RL is also instrumental in advancing autonomous vehicle technology, enhancing the safety and efficiency of self-driving cars, and minimizing accident risks.</p>
<ul>
<li>Enhancing safety and efficiency</li>
<li>Minimising accident risks</li>
</ul></li>
<li><p><strong>Chatbot development.</strong> Additionally, RL is revolutionizing the way chatbots learn, enhancing their conversational skills. This leads to more accurate responses over time, thereby improving user experiences.</p>
<ul>
<li>Enhancing conversational skills</li>
<li>Improving user experiences</li>
</ul></li>
</ol>
</section>
<section id="references" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="references"><span class="header-section-number">2.11</span> References</h2>
<p>The notation and definitions are taken (with small variations) from:</p>
<ul>
<li><span class="citation" data-cites="sutton2018">Richard S. Sutton (<a href="references.html#ref-sutton2018" role="doc-biblioref">2018</a>)</span></li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-sutton2018" class="csl-entry" role="listitem">
Richard S. Sutton, Andrew G. Barto. 2018. <em>Reinforcement Learning. An Introduction.</em> Second edition. MIT Press. <a href="https://github.com/MrinmoiHossain/Reinforcement-Learning-Specialization-Coursera/tree/master">https://github.com/MrinmoiHossain/Reinforcement-Learning-Specialization-Coursera/tree/master</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./sec1.html" class="pagination-link" aria-label="Intelligent Agents and Machines">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Intelligent Agents and Machines</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./sec3.html" class="pagination-link" aria-label="Multi-armed Bandits">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>