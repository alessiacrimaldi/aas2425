[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Autonomous and Adaptive Systems",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Autonomous and Adaptive Systems",
    "section": "Overview",
    "text": "Overview\nThis course will provide the students with a solid understanding of the state of the art and the key conceptual and practical aspects of the design, implementation and evaluation of intelligent machines and autonomous systems that learn by interacting with their environment. The course also takes into consideration ethical, societal and philosophical aspects related to these technologies.\nStrong focus on the state-of-the-art (and what’s next): read papers from leading AI/ML conferences (but also classics from the ML field), try software, etc.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#topics-of-the-module",
    "href": "index.html#topics-of-the-module",
    "title": "Autonomous and Adaptive Systems",
    "section": "Topics of the Module",
    "text": "Topics of the Module",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#interesting-news-and-readings",
    "href": "index.html#interesting-news-and-readings",
    "title": "Autonomous and Adaptive Systems",
    "section": "Interesting News and Readings",
    "text": "Interesting News and Readings\n\nThe Dartmouth Summer Research Project on Artificial Intelligence: Solomonoff (06 May 2023).\n\nAndrew Barto and Richard Sutton Recognized as Pioneers of Reinforcement Learning: Ormond (2024).\nTo catch up with Probability and Computing: Mitzenmacher (2017).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#papers-sources",
    "href": "index.html#papers-sources",
    "title": "Autonomous and Adaptive Systems",
    "section": "Papers’ sources",
    "text": "Papers’ sources\n\nGoogle Scholar (scholar.google.com)\n\n\n\n\n\nMitzenmacher, Michael. 2017. Probability and Computing. Second edition. Cambridge University Press. https://www.cambridge.org/jp/universitypress/subjects/computer-science/algorithmics-complexity-computer-algebra-and-computational-g/probability-and-computing-randomization-and-probabilistic-techniques-algorithms-and-data-analysis-2nd-edition.\n\n\nMusolesi, Mirco. 2024-25. “Autonomous and Adaptive Systems.” https://www.mircomusolesi.org/courses/AAS24-25/AAS24-25-main/.\n\n\nOrmond, Jim. 2024. “2024 Turing Award.” ACM. https://awards.acm.org/about/2024-turing.\n\n\nSolomonoff, Grace. 06 May 2023. “The Meeting of the Minds That Launched AI.” IEEE Spectrum. https://spectrum.ieee.org/dartmouth-ai-workshop.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sec1.html",
    "href": "sec1.html",
    "title": "1  Intelligent Agents and Machines",
    "section": "",
    "text": "The Brave New World.\n\n\n\n\n\n\n\n\n \\(\\Rightarrow \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 2 \\end{bmatrix}\\)  gridworld",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intelligent Agents and Machines</span>"
    ]
  },
  {
    "objectID": "sec2.html",
    "href": "sec2.html",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "",
    "text": "2.1 Examples of Problems",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#finite-markov-decision-processes",
    "href": "sec2.html#finite-markov-decision-processes",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.2 Finite Markov Decision Processes",
    "text": "2.2 Finite Markov Decision Processes\nMarkov Decision Processes (MDPs) are a mathematically idealised formulation of Reinforcement Learning for which precise theoretical statements can be made.\n\nTension between breadth of applicability and mathematical tractability.\nMDPs provide a way for framing the problem of learning from experience, and, more specifically, from interacting with an environment.\n\n\nDefinitions\n\nReinforcement Learning, or RL for short, is a unique facet of machine learning where an agent learns to make decisions through trial and error.\nTwo main entities:\n\n\n\n\nAgent = learner and decision-maker.\n\nInteracts with the environment selecting actions.\nObserves and acts within the environment.\nReceives:\n\nrewards for good decisions.\npenalties for bad decisions.\n\n\nEnvironment = everything else outside the agent.\n\nChanges following actions of the agent.\n\n\n\nGoal: devise a strategy that maximises the total reward over time .\n\n\nRL framework\n\n\nAgent: learner, decision-maker.\nEnvironment: challenges to be solved.\nState: environment snapshot at given time.\nAction: agent’s choice in response to state.\nReward: feedback for agent action (positive or negative).\n\n\n\n\n\nRL interaction loop\n\n\nThe agent and the environment interact at each discrete step of a sequence \\(t=0,1,2,3,\\dots\\)\nAt each time step \\(t\\), the agent receives some representation of the environment state \\(S_t \\in \\mathcal{S}\\) where \\(\\mathcal{S}\\) is the set of the states.\nOn that basis, an agent selects an action \\(A_t \\in \\mathcal{A}(S_t)\\) where \\(\\mathcal{A}(S_t)\\) is the set of the actions that can be taken in state \\(S_t\\).\nAt time \\(t+1\\), as a consequence of its action, the agent receives a reward \\(R_{t+1} \\in \\mathcal{R}\\), where \\(\\mathcal{R}\\) is the set of rewards (expressed as real numbers).\n\n\n\n\nLet’s demonstrate the agent-environment interaction using a generic code example. The process starts by creating an environment and retrieving the initial state. The agent then enters a loop where it selects an action based on the current state in each iteration. After executing the action, the environment provides feedback in the form of a new state and a reward. Finally, the agent updates its knowledge based on the state, action, and reward it received.\n\nenv = create_environment()\nstate = env_get_initial_state()\n\nfor i in range(n_iterations):\n    action = choose_action(state)\n    state, reward = env_execute(action)\n    update_knowledge(state, action, reward)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#goals-and-rewards",
    "href": "sec2.html#goals-and-rewards",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.3 Goals and Rewards",
    "text": "2.3 Goals and Rewards\n\nThe goal of the agent is formalised in terms of the reward it receives.\nAt each time step, the reward is a simple number \\(R_t \\in \\mathbb{R}\\).\nInformally, the agent’s goal is to maximise the total amount it receives.\nThe agent should not maximise the immediate reward, but the cumulative reward.\n\n\nThe Reward Hypothesis\n\nWe can formalise the goal of an agent by stating the “reward hypothesis”:\n\nAll of what we mean by goals and purposes can be well thought of as the maximisation of the exprected value of the cumulative sum of a received scalar signal (reward).\n\n\nExpected Returns\n\nIn RL, actions carry long-term consequences, impacting both immediate and future rewards. The agent’s goal goes beyond maximizing immediate gains; it receives a sequence of rewards and it strives to accumulate the highest total reward over time. This leads us to a key concept in RL: the expected return.\n\nThe expected return \\(G_t\\) is a function of the reward sequence \\(R_{t+1}, R_{t+2}, R_{t+3}, \\dots\\)\n\n\\(G_t\\) is the sum of all rewards the agent expects to accumulate throughout its journey.\n\n\n\n\n\nAccordingly, the agent learns to anticipate the sequence of actions that will yield the highest possible return.\n\nEpisodic Tasks and Continuing Tasks\n\nIn RL, we encounter two types of tasks: episodic and continuous.\nEpisodic tasks are those in which we can identify a final step of the sequence of rewards, i.e. in which the interaction between the agent and the environment can be broken into sub-sequences that we call episodes (such as play of a game, rpeeated tasks, etc.). For example, in a chess game played by an agent, each game constitutes an episode; once a game concludes, the environment resets for the next one.\n\nAn episodic task is divided into distinct episodes, each with a defined beginning and end.\n\nEach episode ends in terminal state after \\(T\\) steps, followed by a reset to a standard starting state or to a sample of a distribution of starting states.\n\n\nThe next episode is completely independent from the previous one.\n\n\nOn the other hand, continuing tasks involve ongoing interaction without distinct episodes (e.g. ongoing process control or robots with a long-lifespan). A typical example is an agent continuously adjusting traffic lights in a city to optimize flow.\n\nA continuing task is one in which it is not possible to identify a final state.\n\n\nExpected Return for Episodic Tasks and Continuing Tasks\n\n\nIn the case of episodic tasks the expected return associated to the selection of an action \\(A_t\\) is the sum of rewards defined as follows:\n\n\\(G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\dots + R_T\\)\n\n\n\nIn the case of continuing tasks the expected return associated to the selection of an action \\(A_t\\) is defined as follows:\n\n\\(G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots  = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\)\n\nwhere \\(\\gamma\\) is the discount rate, with \\(0 \\leq \\gamma \\leq 1\\).\n\n\nDiscounting Rewards\n\nImmediate rewards are typically valued more than future ones, leading to the concept of discounted return. This concept prioritizes more recent rewards by multiplying each reward by a discount factor, gamma, raised to the power of its respective time step. For example, for expected rewards r1 through rn, the discounted return would be calculated as r1 + gamma * r2 + gamma^2 * r3, and so on.\n\n\n\nThe discount factor \\(\\gamma\\), ranging between 0 and 1, is crucial for balancing immediate and long-term rewards. A lower gamma value leads the agent to prioritize immediate gains, while a higher value emphasizes long-term benefits. At the extremes, a gamma of zero means the agent focuses solely on immediate rewards, while a gamma of one considers future rewards as equally important, applying no discount.\n\n\n\n\nNumerical example:\n\nIn this example, we’ll demonstrate how to calculate the discounted_return from an array of expected_rewards. We define a discount_factor of 0.9, then create an array of discounts, where each element corresponds to the discount factor raised to the power of the reward’s position in the sequence.\n\nimport numpy as np\nexpected_rewards = np.array([1, 6, 3])\ndiscount_factor = 0.9\ndiscounts = np.array([discount_factor ** i for i in range(len(expected_rewards))])\nprint(f\"Discounts: {discounts}\")\n\nDiscounts: [1.   0.9  0.81]\n\n\nAs we can see, discounts decrease over time, giving less importance to future rewards. Next, we multiply each reward by its corresponding discount and sum the results to compute the discounted_return, which is 8.83 in this example.\n\ndiscounted_return = np.sum(expected_rewards * discounts)\nprint(f\"The discounted return is {discounted_return}\")\n\nThe discounted return is 8.83\n\n\n\nRelation between Returns at Successive Time Steps\n\n\nReturns at successive time steps are related to each others as follows:\n\n\\(G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots\\) \\(\\quad\\;\\; = R_{t+1} + \\gamma (R_{t+1} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\dots )\\) \\(\\quad\\;\\; = R_{t+1} + \\gamma G_{t+1}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#policies-and-value-functions",
    "href": "sec2.html#policies-and-value-functions",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.4 Policies and Value Functions",
    "text": "2.4 Policies and Value Functions\nAlmost all reinforcement learning algorithms involve estimating value functions, i.e., functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).\nA policy is used to model the behaviour of the agent based on the previous experience and the rewards (and consequently the expected returns) an agent received in the past.\n\nPolicy\n\nFormally, a policy is a mapping from states to probabilities of each possible action, i.e. policy \\(\\pi\\) is a probability distribution.\n\nIf the agent is following policy \\(\\pi\\) at time \\(t\\), then \\(\\pi(a \\vert s)\\) is the probability that \\(A_t = a\\) if \\(S_t = s\\).\n\n\nState-Value Function\n\nThe value function of a state \\(s\\) under a policy \\(\\pi\\), denoted \\(v_\\pi(s)\\), is the expected return when starting in \\(s\\) and following \\(\\pi\\) thereafter.\n\nFor MDPs, we can define the state-value function \\(v_\\pi\\) for policy \\(\\pi\\) formally as:\n\n\\(v_s \\doteq E_\\pi[G_t \\vert S_t = s] = E_\\pi [\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\vert S_t = s]\\)\n\nfor all \\(s \\in \\mathcal{S}\\).\nwhere \\(E_\\pi[.]\\) denotes the expected value of a random variable, given that the agent follows \\(\\pi\\) and \\(t\\) is any time step. The value of the terminal state is 0.\n\n\nchoose = rand(0.1)\nif choose &gt;= 0.7:\n    move left\nelse:\n    move right\n\n\nAction-Value Function",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#choosing-the-rewards",
    "href": "sec2.html#choosing-the-rewards",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.5 Choosing the Rewards",
    "text": "2.5 Choosing the Rewards\n\n\n\nMarvin Minsky. Steps Toward Artificial Intelligence. Proceedings of the IRE. Volume 49. Issue 1. January 1961.\n\n\nExamples of Rewards",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#estimating-value-functions",
    "href": "sec2.html#estimating-value-functions",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.6 Estimating Value Functions",
    "text": "2.6 Estimating Value Functions\n\nMonte-Carlo Methods\n\n\nUsing function approximators (neural networks and deep RL)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#optimal-policies-and-optimal-value-functions",
    "href": "sec2.html#optimal-policies-and-optimal-value-functions",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.7 Optimal Policies and Optimal Value Functions",
    "text": "2.7 Optimal Policies and Optimal Value Functions\n\nDefinition of Optimal Policy\n\n\nOptimal Value Functions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#difference-between-rl-and-other-ml-sub-domains",
    "href": "sec2.html#difference-between-rl-and-other-ml-sub-domains",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.8 Difference between RL and other ML sub-domains",
    "text": "2.8 Difference between RL and other ML sub-domains\nRL differs significantly from other types of machine learning, such as supervised and unsupervised learning.\nIn supervised learning, models are trained using labeled data, learning to predict outcomes based on examples. It is suitable for solving problems like classification and regression.\nUnsupervised learning, on the other hand, involves learning to identify patterns or structures from unlabeled data. It is suitable for solving problems like clustering or association analysis.\nReinforcement Learning, distinct from both, does not use any training data, and learns through trial and error to perform actions that maximize the reward, making it ideal for decision-making tasks.\n\n\n\n\nRL vs. Supervised Learning\n\nSupervised learning is learning from a set of labeled examples and, in interactive problems, it is hard to obtain labels in the first place. Therefore, in “unknown” situations, agents have to learn from their experience. In these situations Reinforcement learning is most beneficial.\n\nRL vs. Unsupervised Learning\n\nUnsupervised learning is learning from datasets containing unlabelled data. Since RL does not rely on examples (labels) of correct behaviour and instead explored and learns it, we may think that RL is a type of unsupervised learning. However, this is not the case because in Reinforcement Learning the goal is to maximise a reward signal instead of trying to find a hidden structure.\nFor this reason, Reinforcement Learning is usually considred a third paradigm in addition to supervised and unsupervised learning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#when-to-use-rl",
    "href": "sec2.html#when-to-use-rl",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.9 When to use RL",
    "text": "2.9 When to use RL\nIn particular, RL is well-suited for scenarios that require training a model to make sequential decisions where each decision influences future observations. In this setting, the agent learns through rewards and penalties. These guide it towards developing more effective strategies without any kind of direct supervision.\n\nSequential decision-making\n\nDecisions influence future observations\n\nLearning through rewards and penalties\n\nNo direct supervision\n\n\n\nAppropriate vs. Inappropriate for RL\n\nAn appropriate example for RL is playing video games, where the player needs to make sequential decisions such as jumping over obstacles or avoiding enemies. The player learns and improves by trial and error, receiving points for successful actions and losing lives for mistakes. The goal is to maximize the score by learning the best strategies to overcome the game’s challenges.\n\n\nPlayer makes sequential decisions.\nReceives points and loses lives depending on actions.\n\nConversely, RL is unsuitable for tasks such as in-game object recognition, where the objective is to identify and classify elements like characters or items in a video frame. This task does not involve sequential decision-making or interaction with an environment. Instead, supervised learning, which employs labeled data to train models in recognizing and categorizing objects, proves more effective for this purpose.\n\n\nNo sequential decision-making.\nNo interaction with an environment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#rl-applications",
    "href": "sec2.html#rl-applications",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.10 RL Applications",
    "text": "2.10 RL Applications\nBeyond its well-known use in gaming, RL has a myriad of applications across various sectors.\n\n   \n\n\nRobotics. In robotics, RL is pivotal for teaching robots tasks through trial and error, like walking or object manipulation.\n\nRobot walking\nObject manipulation\n\nFinance. The finance industry leverages RL for optimizing trading and investment strategies to maximize profit.\n\nOptimizing trading and investment\nMaximise profit\n\nAutonomous Vehicles. RL is also instrumental in advancing autonomous vehicle technology, enhancing the safety and efficiency of self-driving cars, and minimizing accident risks.\n\nEnhancing safety and efficiency\nMinimising accident risks\n\nChatbot development. Additionally, RL is revolutionizing the way chatbots learn, enhancing their conversational skills. This leads to more accurate responses over time, thereby improving user experiences.\n\nEnhancing conversational skills\nImproving user experiences",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec2.html#references",
    "href": "sec2.html#references",
    "title": "2  Introduction to Reinforcement Learning",
    "section": "2.11 References",
    "text": "2.11 References\nThe notation and definitions are taken (with small variations) from:\n\nRichard S. Sutton (2018)\n\n\n\n\n\nRichard S. Sutton, Andrew G. Barto. 2018. Reinforcement Learning. An Introduction. Second edition. MIT Press. https://github.com/MrinmoiHossain/Reinforcement-Learning-Specialization-Coursera/tree/master.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "sec3.html",
    "href": "sec3.html",
    "title": "3  Multi-armed Bandits",
    "section": "",
    "text": "3.1 K-armed Bandit Problem\nThis is the “classic” k-armed bandit problem, which is named by analogy to a slot machine or “one-armed” bandit, except it has \\(k\\) levers instead than one:\nlike in a slot machine, we need to learn which lever provides us with the highest reward.\nIn the case of the k-armed bandit problem, the state is always the same (or, in other words, it does not matter). We can think about having \\(s=\\overline{s}\\) with \\(\\overline{s}\\) constant.\nIn a k-armed bandit problem, each of the \\(k\\) actions has a value, equal to the expected or mean reward given that that action is selected.\nIn the trivial case, since we know the value of each action, solving the k-armed bandit problem is very easy: it is sufficient to always select the action with the highest value!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#k-armed-bandit-problem",
    "href": "sec3.html#k-armed-bandit-problem",
    "title": "3  Multi-armed Bandits",
    "section": "",
    "text": "Problem Definition\n\n\nWe can model a k-armed problem as follows:\n\n\\(k\\) different actions;\nafter each choice we receive a numerical value that depends only on the action we selected;\nthe goal is to maximise the expected reward over a certain number of time steps.\n\n\n\n\n\nThe problem is the same, maximising the expected reward:\n\n\\(E[G_t \\vert S_t = s, A_t = a]\\)\n\\(\\qquad \\;\\; \\doteq E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\vert S_t = s, A_t = a]\\)\n\\(\\qquad \\;\\; = E [\\sum^\\infty_{k=0} \\gamma^k R_{t+k+1} \\vert S_t = s, A_t = a]\\)\n\n\n\n\nConsidering a constant state \\(\\overline{s}\\), the problem is equal to maximise:\n\n\\(E[G_t \\vert S_t = \\overline{s}, A_t = a]\\)\n\\(\\qquad \\;\\; \\doteq E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\vert S_t =  \\overline{s}, A_t = a]\\)\n\\(\\qquad \\;\\; = E [\\sum^\\infty_{k=0} \\gamma^k R_{t+k+1} \\vert S_t = \\overline{s}, A_t = a]\\)\n\nwith \\(a\\) one of the \\(k\\) actions.\n\n\nAction Selection and Estimated Value\n\n\n\nAs in the general RL case, we denote the action selected on time step \\(t\\) as \\(A_t\\) and the corresponding reward as \\(R_t\\).\n\n\nThe value of an arbitrary action \\(a\\) is the expected reward given that \\(a\\) is selected:\n\n\\(q_*(a) \\doteq E[R_t \\vert A_t = a]\\)\n\n\n\n\nHowever, in general, we do not know the action values with certainty, we only know estimates.\n\n\nThe estimated value of action \\(a\\) at time step \\(t\\):\n\n\\(Q_t(a)\\)\n\nIdeally, we would like to have that the value of \\(Q_t(a)\\) would be very close to \\(q_*(a)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#exploration-vs.-exploitation",
    "href": "sec3.html#exploration-vs.-exploitation",
    "title": "3  Multi-armed Bandits",
    "section": "3.2 Exploration vs. Exploitation",
    "text": "3.2 Exploration vs. Exploitation\nWe maintain the estimates of each action value.\n\nExploitation:\n\nAt any step there is at least one action whose estimated value is the greatest \\(\\rightarrow\\) we refer to this action as greedy action.\nWhen we select one of these actions, we are exploiting our current knowledge of the values of the actions.\n\n\n\nExploration:\n\nIf we select non-greedy actions, we say that we are exploring \\(\\rightarrow\\) alternative actions.\nBy doing so, we can improve our estimation of the value functions of the non-greedy actions.\n\n\nBalancing exploration and exploitation in a smart way is the key aspect: Several theoretical resuslts in terms of bounds, etc. given specific assumptions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#evaluating-action-value-methods",
    "href": "sec3.html#evaluating-action-value-methods",
    "title": "3  Multi-armed Bandits",
    "section": "3.3 Evaluating Action-Value Methods",
    "text": "3.3 Evaluating Action-Value Methods\n\nMulti-armed bandits are a very good way of approaching the general problem of Reinforcement Learning.\nSimplification: the state does not change, which means:\n\nOur actions do not modify the state;\nSince the state does not change, the agent’s actions will not depend on the previous actions (the two things are strictly linked).\n\nWe will consider later the “full” reinforcement learning problem where the agent’s actions do modify the state and the agent’s action do depend on the previous actions.\nRecall: the true value of an action is the mean reward when that action is selected.\n\nA possible way to estimate this is by averaging the rewards actually received:\n\n\\(Q_t(a) \\doteq \\frac{\\text{sum of rewards when an action } a \\text{ is taken prior time }t}{\\text{number of times an action } a \\text{ is taken prior time }t}\\) \\[= \\frac{\\sum_{i=1}^{t-1} R_i 1_{A_i=a}}{\\sum_{i=1}^{t-1} 1_{A_i=a}}\\]\n\nwhere \\(1\\) denotes the random variable that is 1 if the predicate \\(A_i=a\\) is true and 0 if not.\nIf the denominator is 0, we set \\(Q_t(a)\\) to some default value (e.g., 0).\n\nAs the denominator goes to infinity, by the law of large numbers, \\(Q_t(a)\\) converges to \\(q_*(a)\\).\nThis is called the sample-average method because each estimate is the average of the sample of the rewards.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#greedy-action-selection-rule",
    "href": "sec3.html#greedy-action-selection-rule",
    "title": "3  Multi-armed Bandits",
    "section": "3.4 Greedy Action Selection Rule",
    "text": "3.4 Greedy Action Selection Rule",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#epsilon-greedy-selection-rule",
    "href": "sec3.html#epsilon-greedy-selection-rule",
    "title": "3  Multi-armed Bandits",
    "section": "3.5 \\(\\epsilon\\)-Greedy Selection Rule",
    "text": "3.5 \\(\\epsilon\\)-Greedy Selection Rule",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#incremental-implementation",
    "href": "sec3.html#incremental-implementation",
    "title": "3  Multi-armed Bandits",
    "section": "3.6 Incremental Implementation",
    "text": "3.6 Incremental Implementation\n\nTracking a Stationary Problem\n\nLet’s now discuss how to implement these methods in practice. We consider a single action \\(a\\) to simplify the notation.\n\n\nLet \\(R_i(a)\\) the reward received after the \\(i\\)-th selection of the action \\(a\\).\nLet \\(Q_n(a)\\) denote the estimate of its action value after it has been selected \\(n-1\\) times.\n\n\nWe can write:\n\\(Q_n(a) \\doteq \\frac{R_1(a) + R_2(a) + \\dots + R_{n-1}(a)}{n-1}\\)\n\n\n\nTrivial impementation\n\nThe trivial implementation would be to maintain a record of all the rewards and then execute the formula when that value would be needed. However, if this is done, the computational requirements would grow over time. A possible alternative is an incremental implementation.\n\nEfficient computation of estimates\n\n\nIncremental implementation:\n\n\\(Q_{n+1} = \\frac{1}{n} \\sum^n_{i=1} R_i\\)\n\\(\\qquad \\;\\; = \\frac{1}{n} (R_n + \\sum^{n-1}_{i=1} R_i)\\)\n\\(\\qquad \\;\\; = \\frac{1}{n} (R_n + (n-1) \\frac{1}{n-1} \\sum^{n-1}_{i=1} R_i)\\)\n\\(\\qquad \\;\\; = \\frac{1}{n} (R_n + (n-1) Q_n)\\)\n\\(\\qquad \\;\\; = \\frac{1}{n} (R_n + n Q_n - Q_n)\\)\n\\(\\qquad \\;\\; = Q_n + \\frac{1}{n} (R_n - Q_n)\\)\n\n\nThis form of update rule is quite common in reinforcement learning.\n\nThe general form is:\n\nNewEstimate \\(\\leftarrow\\) OldEstimate + StepSize (Target – OldEstimate)\n\nwhere the expression (Target – OldEstimate) is usually defined as error in the estimate.\n\n\nTracking a Nonstationary Problem\n\nThe averaging method discussed before is appropriate for stationary bandit problems.\nHowever, many problems are non-stationary. In these cases, it makes sense to give more weight to recent rewards rather than long-past rewards.\nOne of the most popular way of doing this is to use constant step-size parameter (in the example above was \\(\\frac{1}{n}\\)).\n\nThe incremental update rule for updating an average \\(Q_n\\) of the \\(n-1\\) past rewards is modified to be:\n\n\\(Q_{n+1} \\doteq Q_n + \\alpha(R_n - Q_n)\\)\n\nwhere the step-size parameter \\(\\alpha \\in (0,1]\\) is constant.\n\nThis results in \\(Q_{n+1}\\) being a weighted average of past rewards and the initial estimate.\n\nMore formally:\n\n\\(Q_{n+1} \\doteq Q_n + \\alpha (R_n + Q_n)\\)\n\\(\\qquad \\;\\; = (1-\\alpha)^n Q_1 + \\sum^n_{i=1} \\alpha (1-\\alpha)^{(n-i)}R_i\\)\n\n\nThis is called a weighted average since:\n\n\\[(1-\\alpha)^n + \\sum^n_{i=1} \\alpha(1-\\alpha)^{(n-i)} = 1\\]\n\nThe quantity \\(1-\\alpha\\) is less than 1 and the weight given to \\(R_i\\) decreases as the number of rewards increases: actually it’s exponential, and for this reason it is sometimes called exponential-recency-weighted average.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#optimistic-initial-values",
    "href": "sec3.html#optimistic-initial-values",
    "title": "3  Multi-armed Bandits",
    "section": "3.7 Optimistic Initial Values",
    "text": "3.7 Optimistic Initial Values\n\nExample",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#upper-confidence-bound-ucb-action-selection",
    "href": "sec3.html#upper-confidence-bound-ucb-action-selection",
    "title": "3  Multi-armed Bandits",
    "section": "3.8 Upper-Confidence-Bound (UCB) Action Selection",
    "text": "3.8 Upper-Confidence-Bound (UCB) Action Selection",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#multi-armed-bandits-in-practice",
    "href": "sec3.html#multi-armed-bandits-in-practice",
    "title": "3  Multi-armed Bandits",
    "section": "3.9 Multi-armed Bandits in practice",
    "text": "3.9 Multi-armed Bandits in practice\n\nimport numpy as np\nimport random\n\nclass MultiArmedBandit:\n    '''Create a k-armed bandit.\n\n    Each bandit arm must have its reward distribution.\n\n    The bandit has to receive the action: to select one of the k bandits,\n    with possible value from 1 to 10.\n\n    The bandit must have k random generators of two values i,j (with i &lt; j):\n    the min and max of the reward distribution.\n    e.g.:\n        1 3 -&gt; 1.5, ...\n        2 6 -&gt; 2.1, 3.3, ...\n        4 8 -&gt; 4.2, 5.3, 7.8\n    '''\n    def __init__(self, k):\n        self.arms = [self._create_generator() for i in range(k)]\n        for i, arm in enumerate(self.arms):\n            print(f\"arm {i} range: {arm}\")\n\n    def _create_generator(self):\n        lbound, ubound = 1, 10\n\n        a = random.randint(lbound, ubound)\n        b = random.randint(a, ubound)\n\n        return a, b\n\n    def get_reward(self, a: int):\n        assert a &gt;= 0 and a &lt; len(self.arms)\n        return random.uniform(*self.arms[a])\n\nclass ArmChooser:\n    def __init__(self, k: int, eps: float, initial_value: float):\n        self.Q, self.N = [initial_value] * k, [0.] * k\n        self.eps = eps\n\n    def _epsilon_greedy(self):\n        if random.random() &lt; 1 - self.eps:\n            return np.argmax(self.Q)\n        else:\n            return random.randint(0, len(self.Q) - 1)\n    \n    def choose(self):\n        a = self._epsilon_greedy()\n        R = bandit.get_reward(a)\n        print(f\"R: {R}\")\n        self.N[a] += 1\n        self.Q[a] += 1/self.N[a] * (R - self.Q[a])\n        print(f\"Q: {self.Q}\")\n        print (f\"N: {self.N}\")\n\n# Example usage\nk, T, eps, initial_value = 4, 10, 0.1, 10.\nbandit = MultiArmedBandit(k)\nchooser = ArmChooser(k, eps, initial_value)\n\nfor t in range(T):\n    print(f\"step {t}\")\n    chooser.choose()\n\narm 0 range: (6, 7)\narm 1 range: (9, 9)\narm 2 range: (10, 10)\narm 3 range: (3, 7)\nstep 0\nR: 6.375825829021519\nQ: [6.375825829021519, 10.0, 10.0, 10.0]\nN: [1.0, 0.0, 0.0, 0.0]\nstep 1\nR: 9.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 0.0, 0.0]\nstep 2\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 1.0, 0.0]\nstep 3\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 2.0, 0.0]\nstep 4\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 3.0, 0.0]\nstep 5\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 4.0, 0.0]\nstep 6\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 5.0, 0.0]\nstep 7\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 6.0, 0.0]\nstep 8\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 7.0, 0.0]\nstep 9\nR: 10.0\nQ: [6.375825829021519, 9.0, 10.0, 10.0]\nN: [1.0, 1.0, 8.0, 0.0]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#contextual-bandits",
    "href": "sec3.html#contextual-bandits",
    "title": "3  Multi-armed Bandits",
    "section": "3.10 Contextual bandits",
    "text": "3.10 Contextual bandits\n\nPolicy-Based Action Selection\n\n\nAssociative Search\n\n\nExercise:\n\n\nWebsite serving ads with contextual bandits.\n\n\nDesign a system serving ads on a website (e.g. Google Ads) using contextual bandits.\n\n\nAdditional Readings\n\n\n\n\nLihong Li, Wei Chu, John Langford, Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. Proceedings of the 19th International Conference on World Wide Web (WWW 2010): 661–670.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#exercise",
    "href": "sec3.html#exercise",
    "title": "3  Multi-armed Bandits",
    "section": "3.11 Exercise",
    "text": "3.11 Exercise\n\nWebsite serving ads with bandits.\n\n\nDesign a system serving ads on a website (e.g. Google Ads) only using bandits.\n\n\nimport random\n\ndef shuffle_array(arr):\n    random.shuffle(arr)\n\ndef get_random_int(min_val, max_val):\n    return random.randint(min_val, max_val - 1)\n\ndef get_favorite_category(Q):\n    return max(Q, key=Q.get)\n\ndef choose_next_ad(Q, ads, eps):\n    if random.random() &gt; eps:\n        favorite_category = get_favorite_category(Q)\n        category_ads = [ad for ad in ads if ad[1] == favorite_category]\n    else:\n        random_category = random.choice(list(Q.keys()))\n        category_ads = [ad for ad in ads if ad[1] == random_category]\n    \n    return random.choice(category_ads) if category_ads else None\n\ndef display_ads(ad):\n    print(f\"Showing Ad: {ad[0]} (Category: {ad[1]})\")\n\n\nads = [\n    (\"Soccer ⚽️\", \"Sport\"), (\"Basket 🏀\", \"Sport\"), (\"Volleyball 🏐\", \"Sport\"),\n    (\"Banana 🍌\", \"Fruit\"), (\"Apple 🍏\", \"Fruit\"), (\"Orange 🍊\", \"Fruit\"),\n    (\"Cat 🐈‍⬛\", \"Pet\"), (\"Dog 🐕‍🦺\", \"Pet\"), (\"Parrot 🦜\", \"Pet\"),\n    (\"Sneakers 👟\", \"Fashion\"), (\"Sunglasses 🕶️\", \"Fashion\"), (\"Dress 👗\", \"Fashion\"),\n    (\"Paris 🇫🇷\", \"Travel\"), (\"Tokyo 🇯🇵\", \"Travel\"), (\"London 🇬🇧\", \"Travel\")\n]\n\nshuffle_array(ads)\n\nQ = {category: 0 for _, category in ads}\n\neps = 0.2\n\ndisplay_ads(ads[0])\nwhile True:\n    user_input = input(\"Did you like this ad? (y/skip): \").strip().lower()\n    if user_input == 'y':\n        Q[ads[0][1]] += 1\n    elif user_input == 'skip':\n        Q[ads[0][1]] -= 1\n    \n    next_ad = choose_next_ad(Q, ads, eps)\n    if next_ad:\n        ads[0] = next_ad\n        display_ads(next_ad)\n    else:\n        print(\"No more ads to display.\")\n        break",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec3.html#references",
    "href": "sec3.html#references",
    "title": "3  Multi-armed Bandits",
    "section": "3.12 References",
    "text": "3.12 References\nBooks for the bandit:\n\nChapter 2 of Richard S. Sutton (2018)\nTor Lattimore (2020)\n\n\n\n\n\nRichard S. Sutton, Andrew G. Barto. 2018. Reinforcement Learning. An Introduction. Second edition. MIT Press. https://github.com/MrinmoiHossain/Reinforcement-Learning-Specialization-Coursera/tree/master.\n\n\nTor Lattimore, Csaba Szepesvari. 2020. Bandit Algorithms. Cambridge University Press. https://www.cambridge.org/core/books/bandit-algorithms/8E39FD004E6CE036680F90DD0C6F09FC.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "sec4.html",
    "href": "sec4.html",
    "title": "4  Temporal Difference Methods",
    "section": "",
    "text": "4.1 Temporal-Difference Learning",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#temporal-difference-learning",
    "href": "sec4.html#temporal-difference-learning",
    "title": "4  Temporal Difference Methods",
    "section": "",
    "text": "Temporal-difference (TD) methods:\n\nLike Monte Carlo methods, TD methods can learn directly from experience.\nUnlike Monte Carlo methods, TD methods update estimates based in part on other learned estimates, without waiting for the final outcome (we say that “they bootstrap”).\n\nPrediction vs. Control Problems in TD learning:\n\nWe will consider the problem of prediction (TD prediction) first (i.e., we fix a policy \\(\\pi\\) and we try to estimate the value \\(v_\\pi\\) for that given policy).\nThen we will consider the problem of finding an optimal policy (TD control).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#reviewpreliminaries",
    "href": "sec4.html#reviewpreliminaries",
    "title": "4  Temporal Difference Methods",
    "section": "4.2 Review/Preliminaries",
    "text": "4.2 Review/Preliminaries",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#td-prediction",
    "href": "sec4.html#td-prediction",
    "title": "4  Temporal Difference Methods",
    "section": "4.3 TD Prediction",
    "text": "4.3 TD Prediction\n\nMonte Carlo vs. TD for Prediction\n\n\nTD(0) Algorithm and Updates\n\n\nTD Error and Interpretation\n\n\nAdvantages of TD Methods",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#theoretical-basis-of-td0",
    "href": "sec4.html#theoretical-basis-of-td0",
    "title": "4  Temporal Difference Methods",
    "section": "4.4 Theoretical Basis of TD(0)",
    "text": "4.4 Theoretical Basis of TD(0)\n\nConvergence Properties\n\n\nComparison with Dynamic Programming and Monte Carlo",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#on-policy-and-off-policy-control",
    "href": "sec4.html#on-policy-and-off-policy-control",
    "title": "4  Temporal Difference Methods",
    "section": "4.5 On-Policy and Off-Policy Control",
    "text": "4.5 On-Policy and Off-Policy Control",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#sarsa-on-policy-td-control",
    "href": "sec4.html#sarsa-on-policy-td-control",
    "title": "4  Temporal Difference Methods",
    "section": "4.6 Sarsa: On-Policy TD Control",
    "text": "4.6 Sarsa: On-Policy TD Control\n\nTransition from State-Action Pairs\n\n\nSarsa Algortihm and Updates\n\n\nOnline TD(0) Control",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#q-learning-off-policy-td-control",
    "href": "sec4.html#q-learning-off-policy-td-control",
    "title": "4  Temporal Difference Methods",
    "section": "4.7 Q-Learning: Off-Policy TD Control",
    "text": "4.7 Q-Learning: Off-Policy TD Control\n\nQ-Learning Algorithm and Updates\n\n\nConvergence Properties",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#summary",
    "href": "sec4.html#summary",
    "title": "4  Temporal Difference Methods",
    "section": "4.8 Summary",
    "text": "4.8 Summary\n\nRecap of TD Methods\n\n\nFunction Approximation in RL",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#exercise",
    "href": "sec4.html#exercise",
    "title": "4  Temporal Difference Methods",
    "section": "4.9 Exercise",
    "text": "4.9 Exercise\n\nGridworld.\n\n\nSolve the gridworld problem with Sarsa.\nREINFROCEjs - GridWorld: TD\n\n\nimport numpy as np\nimport random\nimport copy\n\nclass GridWorld:\n    def __init__(self, size):\n        self.size = size\n        self.agentPos = [0, 0]\n\n        grid = np.full((size, size), '0', dtype=str)\n        grid[0, 0] = '🤖'\n        grid[size-1, size-1] = \"🏁\"\n        self.grid = grid\n        self.print_grid()\n\n    def print_grid(self):\n        for i, cell in enumerate(self.grid.flatten()):\n            if i % self.size == 0:\n                print()\n            if cell == '🤖':\n                print(f'{cell} ', end = '')\n            else:\n                print(f'{cell}  ', end = '')\n        print()\n\n    def isInsideBoundary(self, pos):\n        return 0 &lt;= pos[0] &lt; self.size and 0 &lt;= pos[1] &lt; self.size\n\n    def nextStep(self, a):\n        nextPos = copy.deepcopy(self.agentPos)\n\n        match a:\n            case 'u':\n                nextPos[1] -= 1\n            case 'd':\n                nextPos[1] += 1\n            case 'r':\n                nextPos[0] += 1\n            case 'l':\n                nextPos[0] -= 1\n            case _:\n                return\n\n        if self.isInsideBoundary(nextPos):\n            self.grid[self.agentPos[1], self.agentPos[0]] = '0'\n            self.grid[nextPos[1], nextPos[0]] = '🤖'\n            self.agentPos = nextPos\n\n        self.print_grid()\n\n\nclass Agent:\n    def __init__(self, size, nActions, eps):\n        self.Q = np.zeros((size, nActions))\n        self.eps = eps\n\n    def _epsilon_greedy(self):\n        if random.random() &lt; 1 - self.eps:\n            return np.argmax(self.Q)\n        else:\n            return random.randint(0, len(self.Q) - 1)\n\n\n# Example usage\nsize, nActions = 10, 4\nworld = GridWorld(size)\nagent = Agent(size, nActions, 0.1)\n\n\n🤖 0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  0  \n0  0  0  0  0  0  0  0  0  🏁",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec4.html#references",
    "href": "sec4.html#references",
    "title": "4  Temporal Difference Methods",
    "section": "4.10 References",
    "text": "4.10 References\n\nSections 6.2 and 9.4 of Richard S. Sutton (2018)\n\n\n\n\n\nRichard S. Sutton, Andrew G. Barto. 2018. Reinforcement Learning. An Introduction. Second edition. MIT Press. https://github.com/MrinmoiHossain/Reinforcement-Learning-Specialization-Coursera/tree/master.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Temporal Difference Methods</span>"
    ]
  },
  {
    "objectID": "sec6.html",
    "href": "sec6.html",
    "title": "6  Value Approximation Methods",
    "section": "",
    "text": "6.1 Approximation Methods\nWe will write \\(\\hat{v}(s,\\mathrm{w}) \\approx v_\\pi(s)\\)\n\\(\\mathrm{w}\\), but more generally it can be a non-linear function.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Value Approximation Methods</span>"
    ]
  },
  {
    "objectID": "sec6.html#value-function-approximation",
    "href": "sec6.html#value-function-approximation",
    "title": "6  Value Approximation Methods",
    "section": "6.2 Value Function Approximation",
    "text": "6.2 Value Function Approximation\nSupervised learning, aquiring the values through exploration.\nWe have:\n\nthe estimation of the value of a state (we’re in state \\(S_t\\))\nthe actual value of the state (we get the reward and we are in state \\(S_{t+1}\\)).\n\nWe’ll use that state \\(S_t\\) as the actual correct value of the output of our Neural Network. We try to improve our estimation, reducing the error.\n– explanation NN –",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Value Approximation Methods</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Mitzenmacher, Michael. 2017. Probability and Computing. Second\nedition. Cambridge University Press. https://www.cambridge.org/jp/universitypress/subjects/computer-science/algorithmics-complexity-computer-algebra-and-computational-g/probability-and-computing-randomization-and-probabilistic-techniques-algorithms-and-data-analysis-2nd-edition.\n\n\nMusolesi, Mirco. 2024-25. “Autonomous and Adaptive\nSystems.” https://www.mircomusolesi.org/courses/AAS24-25/AAS24-25-main/.\n\n\nOrmond, Jim. 2024. “2024 Turing Award.” ACM. https://awards.acm.org/about/2024-turing.\n\n\nRichard S. Sutton, Andrew G. Barto. 2018. Reinforcement Learning. An\nIntroduction. Second edition. MIT Press. https://github.com/MrinmoiHossain/Reinforcement-Learning-Specialization-Coursera/tree/master.\n\n\nSolomonoff, Grace. 06 May 2023. “The Meeting of the Minds That\nLaunched AI.” IEEE Spectrum. https://spectrum.ieee.org/dartmouth-ai-workshop.\n\n\nTor Lattimore, Csaba Szepesvari. 2020. Bandit Algorithms.\nCambridge University Press. https://www.cambridge.org/core/books/bandit-algorithms/8E39FD004E6CE036680F90DD0C6F09FC.",
    "crumbs": [
      "References"
    ]
  }
]