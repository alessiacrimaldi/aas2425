---
title: "Temporal Difference Methods"
format: html
---

## Temporal-Difference Learning

- **Temporal-difference (TD) methods**:

    - Like Monte Carlo methods, TD methods *can learn directly from experience*.

    - Unlike Monte Carlo methods, TD methods *update estimates based in part on other learned estimates*, without waiting for the final outcome (we say that "they *bootstrap*").

- **Prediction vs. Control Problems in TD learning**:

    1. We will consider the problem of prediction (**TD prediction**) first (i.e., we fix a policy $\pi$ and we try to estimate the value $v_\pi$ for that given policy).

    1. Then we will consider the problem of finding an optimal policy (**TD control**).



## Review/Preliminaries



## TD Prediction

<h3>Monte Carlo vs. TD for Prediction</h3>

<h3>TD(0) Algorithm and Updates</h3>

<h3>TD Error and Interpretation</h3>

<h3>Advantages of TD Methods</h3>



## Theoretical Basis of TD(0)

<h3>Convergence Properties</h3>

<h3>Comparison with Dynamic Programming and Monte Carlo</h3>



## On-Policy and Off-Policy Control



## Sarsa: On-Policy TD Control

<h3>Transition from State-Action Pairs</h3>

<h3>Sarsa Algortihm and Updates</h3>

<h3>Online TD(0) Control</h3>



## Q-Learning: Off-Policy TD Control

<h3>Q-Learning Algorithm and Updates</h3>

<h3>Convergence Properties</h3>



## Summary

<h3>Recap of TD Methods</h3>

<h3>Function Approximation in RL</h3>



## Exercise

<h4>Gridworld.</h4>

> Solve the gridworld problem with Sarsa.
>
> [REINFROCEjs - GridWorld: TD](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html)

```{python}
import numpy as np
import random
import copy

# ENVIRONMENT
class GridWorld:
    def __init__(self, size):
        self.size = size
        self.goalPos = [size - 1, size -1]
        self.reset()

    def reset(self):
        """
        Reset the environment for a new episode
        """
        self.agentPos = [0, 0]
        grid = np.full((size, size), '0', dtype=str)
        grid[self.agentPos[0], self.agentPos[1]] = 'ü§ñ'
        grid[self.goalPos[0], self.goalPos[1]] = 'üèÅ'
        self.grid = grid
        self.printGrid()
    
    def getState(self):
        return self.agentPos[0], self.agentPos[1]

    def printGrid(self):
        for i, cell in enumerate(self.grid.flatten()):
            if i % self.size == 0:
                print()
            if cell == 'ü§ñ':
                print(f'{cell} ', end = '')
            else:
                print(f'{cell}  ', end = '')
        print()

    def _isInsideBoundary(self, pos):
        """
        Check if the given position is inside the grid boundaries
        """
        return 0 <= pos[0] < self.size and 0 <= pos[1] < self.size

    def nextStep(self, a):
        nextPos = copy.deepcopy(self.agentPos)

        match a:
            case 'u':
                nextPos[1] -= 1
            case 'd':
                nextPos[1] += 1
            case 'r':
                nextPos[0] += 1
            case 'l':
                nextPos[0] -= 1
            case _:
                return

        if self._isInsideBoundary(nextPos):
            self.grid[self.agentPos[1], self.agentPos[0]] = '0'
            self.grid[nextPos[1], nextPos[0]] = 'ü§ñ'
            self.agentPos = nextPos

        self.printGrid()

        if self.agentPos == self.goalPos:
            return 0 # goal reached
        else:
            return -1 # still moving


# ROBOT
class Agent:
    def __init__(self, size, nActions, eps, alpha=0.1, gamma=0.9):
        self.Q = np.zeros((size, size, nActions)) # Q-table
        self.eps = eps # exploration rate
        self.alpha = alpha # learning rate
        self.gamma = gamma # discount factor
        self.action = ['u', 'd', 'r', 'l'] # 4 possible actions

    def _epsilonGreedy(self, Qs):
        """
        Pick the best action with high probability
        Pick a random action with probability eps
        """
        if random.random() < 1 - self.eps:
            return np.argmax(Qs)
        else:
            return random.randint(0, len(Qs) - 1)

    def chooseAction(self, s_x, s_y):
        """
        Choose an action based on the current state
        """
        actionIdx = self._epsilonGreedy(self.Q[s_x, s_y])
        return actionIdx, self.action[actionIdx]

    def update(self, s, a, r, s_next, a_next):
        x, y = s # current state
        x_next, y_next = s_next # next state
        a_idx, _ = a 
        a_next_idx, _ = a_next
        
        td_target = r + self.gamma * self.Q[x_next, y_next, a_next_idx]
        td_error = td_target - self.Q[x, y, a_idx]
        self.Q[x, y, a_idx] += self.alpha * td_error


# EXAMPLE USAGE
size, nActions = 7, 4
env = GridWorld(size)
agent = Agent(size, nActions, eps=0.1)

# Loop for each episode
num_episodes = 5
for episode in range(1, num_episodes + 1):
    for i in range(5):
        print()
    print(f"--- Episode {episode} ---")
    env.reset()

    s = env.getState()
    a = agent.chooseAction(s[0], s[1])

    # Loop for each step of episode
    step_idx = 0
    while env.getState() != (size - 1, size - 1):
        print()
        print(f"Episode {episode}, Step {step_idx}:")

        r = env.nextStep(a[1])
        s_next = env.getState()
        a_next = agent.chooseAction(s_next[0], s_next[1])

        agent.update(s, a, r, s_next, a_next)
        s, a = s_next, a_next
        step_idx += 1
```



## References

- **Sections 6.2 and 9.4** of @sutton2018